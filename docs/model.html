---

title: Model

keywords: fastai
sidebar: home_sidebar

summary: "This module contains a PyTorch implementation of the Deep Recurrent Survival Analysis model, which is trained on sequence-to-sequence data with binary labels at each time step, where the event always occurs at the final time step. "
description: "This module contains a PyTorch implementation of the Deep Recurrent Survival Analysis model, which is trained on sequence-to-sequence data with binary labels at each time step, where the event always occurs at the final time step. "
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/01_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DRSA" class="doc_header"><code>class</code> <code>DRSA</code><a href="https://github.com/collinprather/drsa/tree/master/drsa/model.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DRSA</code>(<strong><code>n_features</code></strong>:<code>int</code>, <strong><code>hidden_dim</code></strong>:<code>int</code>, <strong><code>n_layers</code></strong>:<code>int</code>, <strong><code>embeddings</code></strong>:<code>List</code>[<code>Embedding</code>], <strong><code>output_size</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>LSTM_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>Linear_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Deep Recurrent Survival Analysis model.
A relatively shallow net, characterized by an LSTM layer followed by a Linear layer.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="DRSA.__init__" class="doc_header"><code>DRSA.__init__</code><a href="https://github.com/collinprather/drsa/tree/master/drsa/model.py#L18" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>DRSA.__init__</code>(<strong><code>n_features</code></strong>:<code>int</code>, <strong><code>hidden_dim</code></strong>:<code>int</code>, <strong><code>n_layers</code></strong>:<code>int</code>, <strong><code>embeddings</code></strong>:<code>List</code>[<code>Embedding</code>], <strong><code>output_size</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>LSTM_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>Linear_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>)</p>
</blockquote>
<p>inputs:</p>
<ul>
<li><code>n_features</code><ul>
<li>size of the input to the LSTM (number of features)</li>
</ul>
</li>
<li><code>hidden_dim</code>:<ul>
<li>size (dimension) of the hidden state in LSTM</li>
</ul>
</li>
<li><code>n_layers</code>:<ul>
<li>number of layers in LSTM</li>
</ul>
</li>
<li><code>embeddings</code>:<ul>
<li>list of nn.Embeddings for each categorical variable</li>
<li>It is assumed the the 1st categorical feature corresponds with the 0th feature,
the 2nd corresponds with the 1st feature, and so on.</li>
</ul>
</li>
<li><code>output_size</code>:<ul>
<li>size of the linear layer's output, which should always be 1, unless altering this model</li>
</ul>
</li>
<li><code>LSTM_dropout</code>:<ul>
<li>percent of neurons in LSTM layer to apply dropout regularization to during training</li>
</ul>
</li>
<li><code>Linear_dropout</code>:<ul>
<li>percent of neurons in linear layer to apply dropout regularization to during training</li>
</ul>
</li>
</ul>

</div>

</div>

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="DRSA.forward" class="doc_header"><code>DRSA.forward</code><a href="https://github.com/collinprather/drsa/tree/master/drsa/model.py#L71" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>DRSA.forward</code>(<strong><code>X</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>input:</p>
<ul>
<li><code>X</code><ul>
<li>input features of shape (batch_size, sequence length, self.n_features)
output:</li>
</ul>
</li>
<li><code>out</code>:<ul>
<li>the DRSA model's predictions at each time step, for each observation in batch</li>
<li>out is of shape (batch_size, sequence_length, 1)</li>
</ul>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

