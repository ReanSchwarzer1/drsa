{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> This module contains a PyTorch implementation of the Deep Recurrent Survival Analysis model, which is trained on sequence-to-sequence data with binary labels at each time step, where the event always occurs at the final time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# To do: make tests that only rely on torch random num generator!\n",
    "# X = torch.load(\"X.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class DRSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Recurrent Survival Analysis model.\n",
    "    A relatively shallow net, characterized by an LSTM layer followed by a Linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        hidden_dim: int,\n",
    "        n_layers: int,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        output_size: int = 1,\n",
    "        LSTM_dropout: float = 0.0,\n",
    "        Linear_dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        * `n_features`\n",
    "            - size of the input to the LSTM (number of features)\n",
    "        * `hidden_dim`:\n",
    "            - size (dimension) of the hidden state in LSTM\n",
    "        * `n_layers`:\n",
    "            - number of layers in LSTM\n",
    "        * `embeddings`:\n",
    "            - list of nn.Embeddings for each categorical variable\n",
    "            - It is assumed the the 1st categorical feature corresponds with the 0th feature,\n",
    "              the 2nd corresponds with the 1st feature, and so on.\n",
    "        * `output_size`:\n",
    "            - size of the linear layer's output, which should always be 1, unless altering this model\n",
    "        * `LSTM_dropout`:\n",
    "            - percent of neurons in LSTM layer to apply dropout regularization to during training\n",
    "        * `Linear_dropout`:\n",
    "            - percent of neurons in linear layer to apply dropout regularization to during training\n",
    "        \"\"\"\n",
    "        super(DRSA, self).__init__()\n",
    "\n",
    "        # hyper params\n",
    "        self.n_features = n_features\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embeddings\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # model architecture\n",
    "        self.lstm = nn.LSTM(\n",
    "            sum([emb.embedding_dim for emb in self.embeddings])\n",
    "            + self.n_features\n",
    "            - len(self.embeddings),\n",
    "            self.hidden_dim,\n",
    "            self.n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=LSTM_dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.linear_dropout = nn.Dropout(p=Linear_dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X: torch.tensor):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        * `X`\n",
    "            - input features of shape (batch_size, sequence length, self.n_features)\n",
    "        output:\n",
    "        * `out`: \n",
    "            - the DRSA model's predictions at each time step, for each observation in batch\n",
    "            - out is of shape (batch_size, sequence_length, 1)\n",
    "        \"\"\"\n",
    "        # concatenating embedding and numeric features\n",
    "        all_embeddings = [\n",
    "            emb(X[:, :, i].long()) for i, emb in enumerate(self.embeddings)\n",
    "        ]\n",
    "        other_features = X[:, :, len(self.embeddings) :]\n",
    "        all_features = torch.cat(all_embeddings + [other_features.float()], dim=-1)\n",
    "\n",
    "        # passing input and hidden into model (hidden initialized as zeros)\n",
    "        out, hidden = self.lstm(all_features.float())\n",
    "\n",
    "        # passing to linear layer to reshape for predictions\n",
    "        out = self.sigmoid(self.linear_dropout(self.fc(out)))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DRSA.__init__\" class=\"doc_header\"><code>DRSA.__init__</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DRSA.__init__</code>(**`n_features`**:`int`, **`hidden_dim`**:`int`, **`n_layers`**:`int`, **`embeddings`**:`List`\\[`Embedding`\\], **`output_size`**:`int`=*`1`*, **`LSTM_dropout`**:`float`=*`0.0`*, **`Linear_dropout`**:`float`=*`0.0`*)\n",
       "\n",
       "inputs:\n",
       "* `n_features`\n",
       "    - size of the input to the LSTM (number of features)\n",
       "* `hidden_dim`:\n",
       "    - size (dimension) of the hidden state in LSTM\n",
       "* `n_layers`:\n",
       "    - number of layers in LSTM\n",
       "* `embeddings`:\n",
       "    - list of nn.Embeddings for each categorical variable\n",
       "    - It is assumed the the 1st categorical feature corresponds with the 0th feature,\n",
       "      the 2nd corresponds with the 1st feature, and so on.\n",
       "* `output_size`:\n",
       "    - size of the linear layer's output, which should always be 1, unless altering this model\n",
       "* `LSTM_dropout`:\n",
       "    - percent of neurons in LSTM layer to apply dropout regularization to during training\n",
       "* `Linear_dropout`:\n",
       "    - percent of neurons in linear layer to apply dropout regularization to during training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DRSA.forward\" class=\"doc_header\"><code>DRSA.forward</code><a href=\"__main__.py#L62\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DRSA.forward</code>(**`X`**:`tensor`)\n",
       "\n",
       "input:\n",
       "* `X`\n",
       "    - input features of shape (batch_size, sequence length, self.n_features)\n",
       "output:\n",
       "* `out`: \n",
       "    - the DRSA model's predictions at each time step, for each observation in batch\n",
       "    - out is of shape (batch_size, sequence_length, 1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DRSA.__init__)\n",
    "show_doc(DRSA.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "n_pitchers=240\n",
    "pitcher_emb_size=2\n",
    "n_teams=31\n",
    "team_emb_size=2\n",
    "pitcher_embeddings = nn.Embedding(n_pitchers, pitcher_emb_size)\n",
    "team_embeddings = nn.Embedding(n_teams, team_emb_size)\n",
    "ptp_embeddings = [pitcher_embeddings, team_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "drsa = DRSA(n_features=17,\n",
    "            hidden_dim=2,\n",
    "            n_layers=2,\n",
    "            embeddings=ptp_embeddings,\n",
    "            output_size=1,\n",
    "            LSTM_dropout=0.1,\n",
    "            Linear_dropout=0.1)\n",
    "# out = drsa(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 57, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
